context:
  name: lm_eval
  version: 0.4.7

package:
  name: ${{ name|lower }}
  version: ${{ version }}

source:
  url: https://pypi.org/packages/source/${{ name[0] }}/${{ name }}/lm_eval-${{ version }}.tar.gz
  sha256: dcbef8722f363f58cfba36b6d783fc6bb17924b24b8da1684bf1ac835866208d

build:
  number: 0
  skip:
    - win
  script: ${{ PYTHON }} -m pip install . -vv --no-deps --no-build-isolation

requirements:
  build:
    - if: build_platform != target_platform
      then:
        - python
        - cross-python_${{ target_platform }}
    - ${{ compiler('cxx') }}
    - ${{ stdlib("c") }}
    - pybind11 >=2.6.2
  host:
    - python
    - pip
    - setuptools
  run:
    - python
    - pytorch
    - accelerate >=0.26.0
    - evaluate
    - datasets >=2.16.0
    - evaluate >=0.4.0
    - jsonlines
    - numexpr
    - peft >=0.2.0
    - pybind11 >=2.6.2
    - pytablewriter
    - rouge-score >=0.0.4
    - sacrebleu >=1.5.0
    - scikit-learn >=0.24.1
    - sqlitedict
    - tqdm-multiprocess
    - transformers >=4.1
    - zstandard
    - dill
    - word2number
    - more-itertools

tests:
  - python:
      imports:
        - lm_eval
      pip_check: true
  - requirements:
    script:
      - lm_eval -h

about:
  summary: A framework for evaluating autoregressive language models
  license: MIT
  license_file: LICENSE.md
  homepage: https://github.com/EleutherAI/lm-evaluation-harness

extra:
  recipe-maintainers:
    - mediocretech
    - hadim
