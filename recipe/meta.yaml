{% set name = "lm_eval" %}
{% set version = "0.4.4" %}

package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/lm_eval-{{ version }}.tar.gz
  sha256: e7d19a7db9c200605268b199514208ac2582ae02e75e0246b5d31e47a918fdf5

build:
  skip: true  # [py2k]
  skip: true  # [win]
  script: {{ PYTHON }} -m pip install . -vv --no-deps --no-build-isolation
  number: 0

requirements:
  build:
    - {{ compiler('cxx') }}
    - {{ stdlib("c") }}
  host:
    - python
    - pip
  run:
    - python
    - datasets >=2.0.0
    - evaluate
    - jsonlines
    - numexpr
    - openai >=0.6.4
    - peft
    - pybind11 >=2.6.2
    - pycountry
    - pytablewriter
    - rouge-score >=0.0.4
    - sacrebleu ==1.5.0
    - scikit-learn >=0.24.1
    - sqlitedict
    - pytorch >=1.7
    - tqdm-multiprocess
    - transformers >=4.1
    - zstandard
    - more-itertools
    - word2number

test:
  imports:
    - lm_eval
  commands:
    - lm_eval -h
    # - pip check
  requires:
    - pip

about:
  home: https://github.com/EleutherAI/lm-evaluation-harness
  summary: A framework for evaluating autoregressive language models
  license: MIT
  license_file: LICENSE.md

extra:
  recipe-maintainers:
    - mediocretech
